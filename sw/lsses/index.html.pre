<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <title>Performing Large Scale Software Engineering Studies</title>
        <meta name="Author" content="Georgios Gousios"/>
        <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
        <link rel="stylesheet" href="../../homepage.css" type="text/css"/>
        <script type="text/javascript" src="../../scripts/shCore.js"></script>
        <script type="text/javascript" src="../../scripts/shBrushSql.js"></script>
        <script type="text/javascript" src="../../scripts/shBrushBash.js"></script>
        <script type="text/javascript" src="../../scripts/shBrushRuby.js"></script>
        <script type="text/javascript" src="../../scripts/shBrushErlang.js"></script>
        <link type="text/css" rel="stylesheet" href="../../styles/shCoreDefault.css"/>
        <script type="text/javascript">
            SyntaxHighlighter.defaults['gutter'] = false;
            SyntaxHighlighter.all();
        </script>
        
        <link rel="shortcut icon" href="favicon.ico"/>
    </head>
    <body>
        <!--incl:MENU--> 
        <div id="body" class="body">
            <h2>Performing Large Scale Software Engineering Studies</h2>

         <p>

         In our work <em>Performing Large Scale Software Engineering
             Studies</em>, we examine why we must perform large scale studies
         and how we can do them using 
         <a href="http://www.sqo-oss.org">Alitheia Core</a>. 
         
         In our work, we used Alitheia Core, <a
             href="http://git-scm.com/">Git</a> revision <a
             href="http://git.sqo-oss.org/?p=sqo-oss.git;a=commit;h=88a6fcc6d15aac911aba710ea8a30e2a9a166443">88a6fcc6d15aac911aba710ea8a30e2a9a166443</a>.
         <em>(Note that after this revision, Alitheia Core data schema changed,
             so the datasets we provide may not work.)</em> 
         Using it, we processed two data sets:
        
        </p>

        <ui>
            
            <li>The <a href=""> Alitheia Core shared dataset</a> This dataset
            was used to conduct the two case studies. It was prepared </li>
            
        <li>A <a href="https://pithos.grnet.gr/pithos/rest/gousiosg@aueb.gr/files/public/testdata.gz">four project</a> dataset, which we distribute hoping
                it will become a standardised testbed for developing new ideas.
                This dataset includes both raw data and Alitheia Core metadata
                from running the first updater stage for the four projects we
                describe in our work (JConvert, Gnome-VFS, Evolution and FreeBSD).

            </li>
        </ui>

        <h3>Using the datasets</h3>
        <p>      

            Both datasets are essentially MySQL dumps. To use them with Alitheia
            Core, do the following (the instructions assume a Unix-like OS) :
        </p>

        <ol>
            <li><a href="http://www.sqo-oss.org/download">Download</a> 
                and <a href="http://www.sqo-oss.org/quickstart">install</a> 
                Alitheia Core. Alternatively, you can download the Alitheia Core
                <a href="http://www.sqo-oss.org/download/vm">virtual machine</a>
                and use it to import the provided data. The virtual machine 
                appliance follows the Open Virtualization Format specification,
                so it may be able to be hosted in (and scale) cloud 
                installations, such as the Amazon EC2 service (untested).
            </li>

            <li>Load either dump in the MySQL database. This can be done using
                the following command:

                <pre class="brush: bash;">
cat dump.sql |mysql -u alitheia -p alitheia                
                </pre>
             
            </li>

            <li>The raw data for the four project dataset need to be placed (or symlinked) at <code>/home/data</code></li>

            <li>Alitheia Core uses the machine's name to register it as a node
                in a cluster installation. Projects are always assigned to a
                cluster node, through an entry in the
                <code>CLUSTERNODE_PROJECT</code> table. This means that by 
                default Alitheia Core will not be able to use the imported data
                as the projects are assigned to another (our) machine. To fix 
                this, start Alitheia Core and open a connection to MySQL. Then
                do 

                <pre class="brush: sql;">
mysql> select * from CLUSTERNODE;
+----------------+------------------+
| CLUSTERNODE_ID | CLUSTERNODE_NAME |
+----------------+------------------+
|              9 | alitheia         |
|              x | your.hostname    |
+----------------+------------------+
mysql> UPDATE CLUSTERNODE_PROJECT set CLUSTERNODE_ID=x;
Query OK, 511 rows affected (0.10 sec)
Rows matched: 538  Changed: 511  Warnings: 0
                </pre>
            </li>

            <li>Alitheia Core should now be able to process the data. To test the
                installation, start Alitheia Core, install a plug-in in the
                plug-ins page, go to the projects page, select a project and click
                Synchronize in the plug-in dialog. Check whether the jobs 
                number is reducing and whether the failed jobs count is 0 or 
                at least very low.
            </li>

        </ol>

        <h3>Replicating the case studies</h3>
        Instructions on how to replicate the experiments described in the paper
        using the provided datasets.

        <h4>Performance measurements</h4>

        To measure the performance of Alitheia Core, we used the raw data from
        the first dataset. To perform the measurements in your environment,
        in turn just <a href="http://www.sqo-oss.org/addproject">add</a> each 
        project and start all updaters at once. 
        For more precise time keeping, one could set the 
        <code>eu.sqooss.log.perf</code> parameter 
        (in the top level 
        <code>pom.xml</code> file) to <code>true</code> and recompile Alitheia
        Core. Detailed performance measurements (per job) can then be found
        in the <code>runner/perf.log</code> file.

        <h4>Electronic Discussions and Software Evolution</h4>

        <p> 
            The first experiment was run on the full repositories of 64 Gnome
            projects and the FreeBSD project. The projects we examined are
            listed in 
            <!--link:this file,files/lsses-heated-projects.txt-->
            The metadata can be found in the Alitheia Core
            shared dataset. To generate the metadata, we mirrored the full
            source code and mailing list data up to Jan 2009, we imported the
            projects one by one, we split the projects among the machines on
            our cluster (by manually assigning projects to machines in the
            database, as shown above) and finally we fired the metadata
            updaters on all machines.  
        </p>

        <p>
            The second step in the experiment execution involves the running of 
            the Discussion Heat plug-in. Again, we triggered the metric 
            synchronisation function for the discussion heat plug-in (using
            the Alitheia Core web interface on each machine on the cluster).
        </p>

        <p> The case study investigates three hypotheses. The first one
            examines the correlation between the number of messages and the
            depth for each thread. We only examine those threads that have more
            than one messages (or equivalently, the depth of the thread is
            non-zero).  To obtain the data, execute the following simple shell
            script: 
        </p>

        <pre class="brush: bash;">
echo "DEPTH NUMBER" |tr ' ' '\t' > results.txt
cat lsses-heated-projects.txt |
while read project; do 
    echo "select max(DEPTH), COUNT(*) \
          from MAILMESSAGE mm, MAILINGLIST ml, STORED_PROJECT sp  \
          where mm.MLIST_ID=ml.MLIST_ID and ml.PROJECT_ID=sp.PROJECT_ID \
          and sp.PROJECT_NAME='$project' group by (THREAD_ID);"|
    mysql -u alitheia -p'alitheia' alitheia -s; 
done |grep -v ^0 >>results.txt
        </pre>

        <p>
            After obtaining the results file, it is a matter of a very simple
            script to extract the Pearson correlation co-efficient with R
        </p>

        <pre class="brush: erlang">
f&lt;-read.table(&quot;results.txt&quot;, header = TRUE)
plot(f$DEPTH,f$NUM)
cor(f$DEPTH, f$NUM, method=&quot;pearson&quot;)
        </pre>

        <p>
            The results of the running of the discussion heat plug-in, 
            used to evaluate the second hypothesis, can be
            obtained (along with the project name) with the following SQL
            query:
        </p>

        <pre class="brush: sql">
select sp.project_name, m.result, mtm.THREAD_ID 
from  ML_THREAD_MEASUREMENT mtm, MAILINGLIST_THREAD mtl, 
      MAILINGLIST ml, STORED_PROJECT sp, METRIC m 
where mtm.METRIC_ID=m.METRIC_ID and mtm.THREAD_ID=mtl.MLTHREAD_ID 
      and mtl.MAILING_LIST_ID=ml.mlist_id 
      and ml.project_id=sp.project_id and m.MNEMONIC="HOTEFFECT";
        </pre>

        <p> 
            The third hypothesis includes a two sample <em>t-test</em>, whose
            aim is to examine whether the cases identified as heated
            discussions exhibit a statistically significant difference in the
            rate of source line intake compared to the average case. The following
            Ruby script reads the results of the previous query from a file (which
            can be simply constructed using the appended script), 
            randomly samples an equal number of
            versions for which there is not a heated discussion going on and
            calculates the rate of line intake for them. 
        </p>

        <pre class="brush: bash">
echo " select sp.project_name, count(RESULT) \
       from METRIC m, ML_THREAD_MEASUREMENT mtm, MAILINGLIST_THREAD mtl, \
            MAILINGLIST ml, STORED_PROJECT sp \
       where m.METRIC_ID=mtm.METRIC_ID and m.MNEMONIC="HOTEFFECT" \
             and mtm.THREAD_ID=mtl.MLTHREAD_ID \
             and mtl.MAILING_LIST_ID=ml.mlist_id \ 
             and ml.project_id=sp.project_id  \
       group by sp.project_name;" |
mysql -u alitheia -p'alitheia' -s alitheia > heated.txt         
        </pre>


        <pre class="brush: ruby;">
#!/usr/bin/ruby

require 'rubygems'
require 'mysql2'

$mysql = Mysql2::Client.new(:host =&gt; &quot;localhost&quot;, :username =&gt; &quot;alitheia&quot;, 
                            :password =&gt; &quot;alitheia&quot;, :database =&gt; &quot;alitheia&quot;)

MAXPV = &quot;select max(pv.PROJECT_VERSION_ID) 
from PROJECT_VERSION pv, STORED_PROJECT sp 
where pv.STORED_PROJECT_ID=sp.PROJECT_ID and sp.PROJECT_NAME='%s';&quot;

MINPV = &quot;select min(pv.PROJECT_VERSION_ID) 
from PROJECT_VERSION pv, STORED_PROJECT sp 
where pv.STORED_PROJECT_ID=sp.PROJECT_ID and sp.PROJECT_NAME='%s';&quot;

CHECKPV = &quot;select * 
from PROJECT_VERSION pv, STORED_PROJECT sp  
where pv.STORED_PROJECT_ID=sp.PROJECT_ID and sp.PROJECT_NAME='%s' and pv.PROJECT_VERSION_ID=%d;&quot;

VERTS = &quot;select pv.TIMESTAMP as ts 
from PROJECT_VERSION pv 
where pv.PROJECT_VERSION_ID=%d&quot;

VERBETWEEN = &quot;select pv.PROJECT_VERSION_ID as id 
from PROJECT_VERSION pv, STORED_PROJECT sp 
where pv.TIMESTAMP &gt;= %d and pv.TIMESTAMP &lt;= %d 
      and sp.PROJECT_ID=pv.STORED_PROJECT_ID 
      and sp.PROJECT_NAME='%s';&quot;

VERLOC = &quot;select pvm.RESULT as result
from PROJECT_VERSION_MEASUREMENT pvm, METRIC m
where pvm.METRIC_ID=m.METRIC_ID and pvm.PROJECT_VERSION_ID=%d and m.MNEMONIC='VERLOC';&quot;

def results_period (period)
  period.collect{|id|         
    result = $mysql.query(sprintf(VERLOC, id))
    unless result.first.nil?          
      result.first[&quot;result&quot;]
    end
  }
end

srand(0xdeadbabe)
File.open(&quot;heated.txt&quot;).each do |line|
    samples, project = line.split(' ') 
    max = $mysql.query(sprintf(MAXPV, project)).first[&quot;max(pv.PROJECT_VERSION_ID)&quot;]
    min = $mysql.query(sprintf(MINPV, project)).first[&quot;min(pv.PROJECT_VERSION_ID)&quot;]
    
    smpl_array = Array.new
    #Get an array of random version ids from the project
    while smpl_array.length &lt; samples.to_i
      
      rnd = min + rand(max - min)
      
      unless $mysql.query(sprintf(CHECKPV, project, rnd)).count == 0
        smpl_array.push(rnd)
      end
    end
   
    smpl_array.each do |pvid|
      ts = $mysql.query(sprintf(VERTS, pvid)).first[&quot;ts&quot;]
      prev_month = ts - 3600 * 24 * 30 * 1000
      next_week = ts + 3600 * 24 * 7 * 1000

      ver_prev_month = $mysql.query(sprintf(VERBETWEEN, prev_month, ts, project)).collect{|row| row[&quot;id&quot;]}
      ver_next_week  = $mysql.query(sprintf(VERBETWEEN, ts, next_week, project)).collect{|row| row[&quot;id&quot;]}
      
      meas_prev_month = results_period(ver_prev_month) 
      meas_next_week  = results_period(ver_next_week)

      print(project, &quot; &quot;, 
      	meas_prev_month.inject(0.0){|sum, n| sum + n.to_f} / meas_prev_month.size -
        meas_next_week.inject(0.0){|sum, n| sum + n.to_f} / meas_next_week.size, 
        &quot;\n&quot;)
    end
end
    
        </pre>

        <p>
            After having obtained the results of both the metric (in file 
            <code>heat-result.txt</code>) and the Ruby script (in file
            <code>t-test-sample.txt</code>) it is straightforward to do
            the <em>t-test</em> in R:
        </p>

        <pre class="brush: erlang;">
d&lt;-read.table(&quot;t-test-sample.txt&quot;, header = TRUE)
f&lt;-read.table(&quot;heat-result.txt&quot;, header = TRUE)
t.test(f$RESULT, d$RESULT)
        </pre>

        <h4>Development Teams and Maintainability</h4>
        
        <p>
            The second case study was performed on 213 projects whose primary
            language (as can be obtained by counting the number of files in 
            their latest version) is C (listed in 
            <!--link:this file,files/lsses-mi-projects-c.txt-->). 
            For each project, we addded it to 
            the Alitheia Core system, assigned it manually to the most 
            appropriate node in our cluster and started the source code metadata
            updater. After the project data were imported, we run the required
            metrics. At the time the experiment took place, Alitheia Core was
            not able to handle metric dependencies, so the order of metric execution
            was preserved by manually initiating metric runs. Later versions, 
            such as the one linked above, will automatically
            run metrics and their dependencies in the correct order.
        </p>

        <p>
            To do the correlation analysis required in order to examine the two
            hypotheses, we obtained results from both the maintainability
            index and the developer metrics plug-ins, for each type of entity 
            we were interested about (specifically, project versions and modules).
            The query to obtain the results at the module level for project 
            <code>Gnome-VFS</code> is the following:
        </p>
 
        <pre class="brush: sql;">
select MI.MODMI, EYEBALL.MODEYEBALL
from
(  select pfm.PROJECT_FILE_ID as ID, pfm.RESULT as MODMI 
   from   STORED_PROJECT sp, PROJECT_VERSION pv, PROJECT_FILE pf, 
          PROJECT_FILE_MEASUREMENT pfm, METRIC m 
   where  pfm.METRIC_ID=m.METRIC_ID and pf.PROJECT_FILE_ID=pfm.PROJECT_FILE_ID 
          and pv.PROJECT_VERSION_ID=pf.PROJECT_VERSION_ID 
          and pv.STORED_PROJECT_ID=sp.PROJECT_ID 
          and m.MNEMONIC="MODMI" and sp.PROJECT_NAME="Gnome-VFS"
) as MI,
(  select pfm.PROJECT_FILE_ID as ID, pfm.RESULT as MODEYEBALL 
   from   STORED_PROJECT sp, PROJECT_VERSION pv, PROJECT_FILE pf, 
          PROJECT_FILE_MEASUREMENT pfm, METRIC m 
   where  pfm.METRIC_ID=m.METRIC_ID and pf.PROJECT_FILE_ID=pfm.PROJECT_FILE_ID 
          and pv.PROJECT_VERSION_ID=pf.PROJECT_VERSION_ID 
          and pv.STORED_PROJECT_ID=sp.PROJECT_ID 
          and m.MNEMONIC="MODEYBALL" 
          and exists (
              select pfm1.PROJECT_FILE_ID 
              from PROJECT_FILE_MEASUREMENT pfm1, METRIC m1 
              where  pfm1.METRIC_ID=m1.METRIC_ID and m1.MNEMONIC="ISSRCMOD" 
                     and pfm.PROJECT_FILE_ID=pfm1.PROJECT_FILE_ID
                     and pfm1.RESULT="1" 
              ) 
          and sp.PROJECT_NAME="Gnome-VFS"
) as EYEBALL
where
    MI.ID = EYEBALL.ID
        </pre>

        <p>
            The query to obtain the results at the project level using a 3 month
            time window to determine team size (team size comprises of developers
            that have committed at least once within this time window) for
            the same project is the following:
        </p>
        <pre class="brush: sql;">
select MI.MI, TEAMSIZE.TEAMSIZE
from
(  select pvm.PROJECT_VERSION_ID as ID, pvm.RESULT as MODMI 
   from   STORED_PROJECT sp, PROJECT_VERSION pv, 
          PROJECT_VERSION_MEASUREMENT pvm, METRIC m 
   where  pvm.METRIC_ID=m.METRIC_ID 
          and pv.PROJECT_VERSION_ID=pvm.PROJECT_VERSION_ID 
          and pv.STORED_PROJECT_ID=sp.PROJECT_ID 
          and m.MNEMONIC="MI" and sp.PROJECT_NAME="Gnome-VFS"
) as MI,
(  select pvm.PROJECT_VERSION_ID as ID, pvm.RESULT as TEAMSIZE
   from   STORED_PROJECT sp, PROJECT_VERSION pv, 
          PROJECT_VERSION_MEASUREMENT pvm, METRIC m 
   where  pvm.METRIC_ID=m.METRIC_ID 
          and pv.PROJECT_VERSION_ID=pvm.PROJECT_VERSION_ID 
          and pv.STORED_PROJECT_ID=sp.PROJECT_ID 
          and m.MNEMONIC="TEAMSIZE3" 
          and sp.PROJECT_NAME="Gnome-VFS"
) as TEAMSIZE
where
    MI.ID = TEAMSIZE.ID

        </pre>

        <p>
            Results at the project and module level for all projects can be obtained
            by means of simple shell scripts that execute the two queries for all
            projects in the 
            <!--link:provided project file list,files/lsses-mi-projects-c.txt-->).
            Then, importing the data in R and performing the correlation analysis
            is as simple as the following script. As the dataset to be analyzed is
            very big, R will need a machine with at least 4 gigabytes of memory
            to process it.
        </p>

        <pre class="brush: erlang">
f&lt;-read.table(&quot;results.txt&quot;, header = TRUE)
cor(f$MI, f$TEAMSIZE, method=&quot;pearson&quot;)
        </pre>


    </div>
    <!--incl:FOOTER-->
</body>
</html>
